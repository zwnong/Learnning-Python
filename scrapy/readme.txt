
大部分爬虫都是按“发送请求——获得页面——解析页面——抽取并储存内容”这样的流程来进行，这其实也是模拟了我们使用浏览器获取网页信息的过程。

scrapy 是一个功能非常强大的爬虫框架，它不仅能便捷地构建request，还有强大的 selector 能够方便地解析 response，然而它最让人惊喜的还是它超高的性能，让你可以将爬虫工程化、模块化。学会 scrapy，你可以自己去搭建一些爬虫框架，你就基本具备爬虫工程师的思维了。

当爬取基本数据已经不是问题了，你的瓶颈会集中到爬取海量数据的效率。这个时候，相信你会很自然地接触到一个很厉害的名字：分布式爬虫。

分布式这个东西，听起来很恐怖，但其实就是利用多线程的原理让多个爬虫同时工作，需要你掌握 Scrapy + MongoDB + Redis 这三种工具。Scrapy前面我们说过了，用于做基本的页面爬取，MongoDB用于存储爬取的数据，Redis则用来存储要爬取的网页队列，也就是任务队列。

以上就是关于学习Python爬虫需要掌握的一些基本知识点，另外还需要注意网站的反爬虫机制，比如动态加载、封IP、验证码、userAgent访问限制等等。面对这些反爬虫，我们也是需要利用一些工具来应对的，比如IP限制可以使用黑洞代理换IP的工具来切换IP地址，这样就能突破IP限制了。